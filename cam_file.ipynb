{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1745246645.210144  936928 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M3 Pro\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1745246645.218382  937086 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745246645.221626  937086 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 8 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting attention detection with 5-second analysis intervals...\n",
      "Press 'q' to quit\n",
      "\n",
      "\n",
      "Analysis at 2025-04-21 20:14:10:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1745246651.080826  937089 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n",
      "2025-04-21 20:14:11.351 python[25653:936928] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-04-21 20:14:11.351 python[25653:936928] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analysis at 2025-04-21 20:14:15:\n",
      "Student 1: Attentive (3/3 models agree)\n",
      "\n",
      "Analysis at 2025-04-21 20:14:20:\n",
      "Student 1: Attentive (2/3 models agree)\n",
      "\n",
      "Analysis at 2025-04-21 20:14:25:\n",
      "Student 1: Attentive (2/3 models agree)\n",
      "\n",
      "Analysis at 2025-04-21 20:14:30:\n",
      "Student 1: Attentive (2/3 models agree)\n",
      "\n",
      "Analysis at 2025-04-21 20:14:35:\n",
      "Student 1: Attentive (2/3 models agree)\n",
      "\n",
      "Analysis at 2025-04-21 20:14:40:\n",
      "Student 1: Attentive (2/3 models agree)\n",
      "\n",
      "Analysis at 2025-04-21 20:14:45:\n",
      "Student 1: Attentive (2/3 models agree)\n",
      "\n",
      "Analysis at 2025-04-21 20:14:50:\n",
      "Student 1: Attentive (2/3 models agree)\n",
      "\n",
      "Analysis at 2025-04-21 20:14:55:\n",
      "Student 1: Attentive (2/3 models agree)\n",
      "\n",
      "Analysis at 2025-04-21 20:15:00:\n",
      "Student 1: Attentive (2/3 models agree)\n",
      "\n",
      "Analysis at 2025-04-21 20:15:05:\n",
      "Student 1: Attentive (2/3 models agree)\n",
      "\n",
      "Analysis at 2025-04-21 20:15:10:\n",
      "Student 1: Attentive (2/3 models agree)\n",
      "\n",
      "Analysis at 2025-04-21 20:15:15:\n",
      "Student 1: Attentive (2/3 models agree)\n",
      "\n",
      "Analysis at 2025-04-21 20:15:20:\n",
      "Student 1: Attentive (2/3 models agree)\n",
      "\n",
      "Analysis at 2025-04-21 20:15:25:\n",
      "\n",
      "Analysis at 2025-04-21 20:15:30:\n",
      "Student 1: Attentive (2/3 models agree)\n",
      "\n",
      "Analysis at 2025-04-21 20:15:35:\n",
      "Student 1: Attentive (2/3 models agree)\n",
      "\n",
      "Analysis at 2025-04-21 20:15:40:\n",
      "Student 1: Attentive (2/3 models agree)\n",
      "\n",
      "Analysis at 2025-04-21 20:15:45:\n",
      "Student 1: Attentive (2/3 models agree)\n",
      "\n",
      "Analysis at 2025-04-21 20:15:50:\n",
      "Student 1: Attentive (2/3 models agree)\n",
      "\n",
      "Analysis at 2025-04-21 20:15:55:\n",
      "Student 1: Attentive (2/3 models agree)\n",
      "\n",
      "Analysis at 2025-04-21 20:16:00:\n",
      "Student 1: Attentive (2/3 models agree)\n",
      "\n",
      "Analysis at 2025-04-21 20:16:06:\n",
      "Student 1: Attentive (2/3 models agree)\n",
      "\n",
      "Analysis at 2025-04-21 20:16:11:\n",
      "Student 1: Distracted (2/3 models agree)\n",
      "\n",
      "Analysis at 2025-04-21 20:16:16:\n",
      "Student 1: Attentive (2/3 models agree)\n",
      "\n",
      "Analysis at 2025-04-21 20:16:21:\n",
      "Student 1: Attentive (2/3 models agree)\n",
      "\n",
      "Analysis at 2025-04-21 20:16:26:\n",
      "Student 1: Attentive (2/3 models agree)\n",
      "\n",
      "Analysis at 2025-04-21 20:16:31:\n",
      "Student 1: Attentive (2/3 models agree)\n",
      "\n",
      "Final Analysis Results:\n",
      "\n",
      "2025-04-21 20:14:10:\n",
      "\n",
      "2025-04-21 20:14:15:\n",
      "  Student 1: Attentive\n",
      "\n",
      "2025-04-21 20:14:20:\n",
      "  Student 1: Attentive\n",
      "\n",
      "2025-04-21 20:14:25:\n",
      "  Student 1: Attentive\n",
      "\n",
      "2025-04-21 20:14:30:\n",
      "  Student 1: Attentive\n",
      "\n",
      "2025-04-21 20:14:35:\n",
      "  Student 1: Attentive\n",
      "\n",
      "2025-04-21 20:14:40:\n",
      "  Student 1: Attentive\n",
      "\n",
      "2025-04-21 20:14:45:\n",
      "  Student 1: Attentive\n",
      "\n",
      "2025-04-21 20:14:50:\n",
      "  Student 1: Attentive\n",
      "\n",
      "2025-04-21 20:14:55:\n",
      "  Student 1: Attentive\n",
      "\n",
      "2025-04-21 20:15:00:\n",
      "  Student 1: Attentive\n",
      "\n",
      "2025-04-21 20:15:05:\n",
      "  Student 1: Attentive\n",
      "\n",
      "2025-04-21 20:15:10:\n",
      "  Student 1: Attentive\n",
      "\n",
      "2025-04-21 20:15:15:\n",
      "  Student 1: Attentive\n",
      "\n",
      "2025-04-21 20:15:20:\n",
      "  Student 1: Attentive\n",
      "\n",
      "2025-04-21 20:15:25:\n",
      "\n",
      "2025-04-21 20:15:30:\n",
      "  Student 1: Attentive\n",
      "\n",
      "2025-04-21 20:15:35:\n",
      "  Student 1: Attentive\n",
      "\n",
      "2025-04-21 20:15:40:\n",
      "  Student 1: Attentive\n",
      "\n",
      "2025-04-21 20:15:45:\n",
      "  Student 1: Attentive\n",
      "\n",
      "2025-04-21 20:15:50:\n",
      "  Student 1: Attentive\n",
      "\n",
      "2025-04-21 20:15:55:\n",
      "  Student 1: Attentive\n",
      "\n",
      "2025-04-21 20:16:00:\n",
      "  Student 1: Attentive\n",
      "\n",
      "2025-04-21 20:16:06:\n",
      "  Student 1: Attentive\n",
      "\n",
      "2025-04-21 20:16:11:\n",
      "  Student 1: Distracted\n",
      "\n",
      "2025-04-21 20:16:16:\n",
      "  Student 1: Attentive\n",
      "\n",
      "2025-04-21 20:16:21:\n",
      "  Student 1: Attentive\n",
      "\n",
      "2025-04-21 20:16:26:\n",
      "  Student 1: Attentive\n",
      "\n",
      "2025-04-21 20:16:31:\n",
      "  Student 1: Attentive\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "class IntervalAttentionDetector:\n",
    "    def __init__(self, max_faces=5, analysis_interval=5):\n",
    "        self.max_faces = max_faces\n",
    "        self.analysis_interval = analysis_interval  # seconds\n",
    "        self.last_analysis_time = 0\n",
    "        \n",
    "        # Initialize MediaPipe Face Mesh\n",
    "        self.face_mesh = mp.solutions.face_mesh.FaceMesh(\n",
    "            max_num_faces=max_faces,\n",
    "            refine_landmarks=False,\n",
    "            min_detection_confidence=0.7,\n",
    "            min_tracking_confidence=0.7\n",
    "        )\n",
    "        \n",
    "        # Load pre-trained models with thresholds\n",
    "        self.models = {\n",
    "            'vgg16': load_model('best_model_vgg16.keras'),\n",
    "            'resnet50': load_model('best_model_resnet50.keras'),\n",
    "            'inceptionv3': load_model('best_model_inceptionv3.keras')\n",
    "        }\n",
    "        self.thresholds = {'vgg16': 0.34, 'resnet50': 0.60, 'inceptionv3': 0.50}\n",
    "        \n",
    "        # Initialize video capture\n",
    "        self.cap = cv2.VideoCapture(0)\n",
    "        self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "        self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "        \n",
    "        # Analysis results storage\n",
    "        self.results = []\n",
    "        self.current_status = [\"Analyzing...\" for _ in range(max_faces)]\n",
    "        \n",
    "        # Performance tracking\n",
    "        self.fps = 0\n",
    "        self.frame_count = 0\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def preprocess_face(self, frame, landmarks):\n",
    "        \"\"\"Crop and prepare face for model input\"\"\"\n",
    "        h, w = frame.shape[:2]\n",
    "        x_coords = [lm.x * w for lm in landmarks.landmark]\n",
    "        y_coords = [lm.y * h for lm in landmarks.landmark]\n",
    "        x_min, x_max = int(min(x_coords)), int(max(x_coords))\n",
    "        y_min, y_max = int(min(y_coords)), int(max(y_coords))\n",
    "        \n",
    "        # Add 50% margin\n",
    "        margin = 0.50\n",
    "        x_min = max(0, x_min - int((x_max - x_min) * margin))\n",
    "        y_min = max(0, y_min - int((y_max - y_min) * margin))\n",
    "        x_max = min(w, x_max + int((x_max - x_min) * margin))\n",
    "        y_max = min(h, y_max + int((y_max - y_min) * margin))\n",
    "        \n",
    "        face_img = frame[y_min:y_max, x_min:x_max]\n",
    "        if face_img.size == 0:\n",
    "            return None\n",
    "            \n",
    "        return cv2.resize(face_img, (256, 256)).astype('float32') / 255.0\n",
    "\n",
    "    def analyze_frame(self, frame):\n",
    "        \"\"\"Perform analysis on captured frame\"\"\"\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = self.face_mesh.process(rgb_frame)\n",
    "        analysis_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        frame_results = {\"time\": analysis_time, \"students\": []}\n",
    "        \n",
    "        if results.multi_face_landmarks:\n",
    "            for i, face_landmarks in enumerate(results.multi_face_landmarks[:self.max_faces]):\n",
    "                face_img = self.preprocess_face(frame, face_landmarks)\n",
    "                if face_img is None:\n",
    "                    continue\n",
    "                \n",
    "                # Ensemble prediction\n",
    "                predictions = []\n",
    "                for name, model in self.models.items():\n",
    "                    pred = model.predict(np.expand_dims(face_img, axis=0), verbose=0)[0][0]\n",
    "                    predictions.append(1 if pred > self.thresholds[name] else 0)\n",
    "                \n",
    "                # Majority vote\n",
    "                final_pred = max(set(predictions), key=predictions.count)\n",
    "                status = \"Distracted\" if final_pred == 1 else \"Attentive\"\n",
    "                self.current_status[i] = status\n",
    "                \n",
    "                # Store results\n",
    "                frame_results[\"students\"].append({\n",
    "                    \"id\": i+1,\n",
    "                    \"status\": status,\n",
    "                    \"models_voted\": f\"{predictions.count(final_pred)}/3\"\n",
    "                })\n",
    "        \n",
    "        self.results.append(frame_results)\n",
    "        print(f\"\\nAnalysis at {analysis_time}:\")\n",
    "        for student in frame_results[\"students\"]:\n",
    "            print(f\"Student {student['id']}: {student['status']} ({student['models_voted']} models agree)\")\n",
    "\n",
    "    def process_frame(self):\n",
    "        \"\"\"Main processing loop\"\"\"\n",
    "        ret, frame = self.cap.read()\n",
    "        if not ret:\n",
    "            return None\n",
    "        \n",
    "        # Calculate FPS\n",
    "        self.frame_count += 1\n",
    "        elapsed_time = time.time() - self.start_time\n",
    "        if elapsed_time > 1:\n",
    "            self.fps = self.frame_count / elapsed_time\n",
    "            self.start_time = time.time()\n",
    "            self.frame_count = 0\n",
    "        \n",
    "        # Check if it's time to analyze\n",
    "        current_time = time.time()\n",
    "        if current_time - self.last_analysis_time >= self.analysis_interval:\n",
    "            self.analyze_frame(frame.copy())  # Analyze a copy of the frame\n",
    "            self.last_analysis_time = current_time\n",
    "        \n",
    "        # Real-time face detection (for visualization only)\n",
    "        small_frame = cv2.resize(frame, (320, 240))\n",
    "        rgb_small = cv2.cvtColor(small_frame, cv2.COLOR_BGR2RGB)\n",
    "        results = self.face_mesh.process(rgb_small)\n",
    "        \n",
    "        if results.multi_face_landmarks:\n",
    "            for i, face_landmarks in enumerate(results.multi_face_landmarks[:self.max_faces]):\n",
    "                nose = face_landmarks.landmark[1]\n",
    "                x = int(nose.x * frame.shape[1])\n",
    "                y = int(nose.y * frame.shape[0])\n",
    "                \n",
    "                # Display current status (from last analysis)\n",
    "                status = self.current_status[i] if i < len(self.current_status) else \"Unknown\"\n",
    "                color = (0, 255, 0) if status == \"Attentive\" else (0, 0, 255)\n",
    "                \n",
    "                cv2.putText(frame, f\"Student {i+1}: {status}\", (x, y - 10),\n",
    "                          cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "                \n",
    "                # Display next analysis time\n",
    "                next_analysis = self.analysis_interval - (current_time - self.last_analysis_time)\n",
    "                cv2.putText(frame, f\"Next in: {max(0, int(next_analysis))}s\", \n",
    "                           (x, y + 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "        \n",
    "        # Display info\n",
    "        cv2.putText(frame, f\"FPS: {self.fps:.1f}\", (10, 30),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "        cv2.putText(frame, f\"Interval: {self.analysis_interval}s\", (10, 60),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "        \n",
    "        return frame\n",
    "\n",
    "    def run(self):\n",
    "        try:\n",
    "            print(f\"Starting attention detection with {self.analysis_interval}-second analysis intervals...\")\n",
    "            print(\"Press 'q' to quit\\n\")\n",
    "            \n",
    "            while True:\n",
    "                frame = self.process_frame()\n",
    "                if frame is None:\n",
    "                    break\n",
    "                \n",
    "                cv2.imshow(\"Interval Attention Detection\", frame)\n",
    "                if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                    break\n",
    "        finally:\n",
    "            self.cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            print(\"\\nFinal Analysis Results:\")\n",
    "            for result in self.results:\n",
    "                print(f\"\\n{result['time']}:\")\n",
    "                for student in result[\"students\"]:\n",
    "                    print(f\"  Student {student['id']}: {student['status']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    detector = IntervalAttentionDetector(max_faces=5, analysis_interval=5)\n",
    "    detector.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
